{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython2"
  },
  "name": "",
  "signature": "sha256:62af76deeb395fd4c53aa50b9d9746411b8bd7597db39beb17cf08008d1352af"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Demo: Vowpal Wabbit, create topic model and infer topics on hold-out"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import bz2\n",
      "\n",
      "import gensim\n",
      "import numpy as np\n",
      "import scipy as sp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Read matrices"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def read_vw_matrix(filename, topics=False, n_term=None):\n",
      "    with open(filename) as f:\n",
      "        if topics:\n",
      "            for i in xrange(11): f.readline()\n",
      "        result_matrix = []\n",
      "        for line in f:\n",
      "            parts = line.strip().replace('  ', ' ').split(' ')\n",
      "            if topics:\n",
      "                index = int(parts[0])\n",
      "                matrix_line = map(float, parts[1:])\n",
      "                if index < n_term or not n_term:\n",
      "                    result_matrix.append(matrix_line)\n",
      "            else:\n",
      "                index = int(parts[-1])\n",
      "                matrix_line = map(float, parts[:-1])\n",
      "                result_matrix.append(matrix_line)\n",
      "    return np.array(result_matrix, dtype=float)\n",
      "\n",
      "def read_vw_gammas(predictions_path):\n",
      "    gammas = read_vw_matrix(predictions_path, topics=False)\n",
      "    return gammas\n",
      "\n",
      "def read_vw_lambdas(topics_path, n_term=None):\n",
      "    lambdas = read_vw_matrix(topics_path, topics=True, n_term=n_term).T\n",
      "    return lambdas"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Infer factorization given Dirichlet distributions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def infer_factorization(alphas, a=0, tol=1e-10):\n",
      "    \"\"\"\n",
      "    Infer stochastic matrix from Dirichlet distributions. \n",
      "    \n",
      "    Input: matrix with rows corresponding to parameters of \n",
      "    the asymmetric Dirichlet distributions, parameter a.\n",
      "    \n",
      "    a=0 => expected distributions\n",
      "    a=1 => most probable distributions\n",
      "    a=1/2 => normalized median-marginal distributions\n",
      "    \n",
      "    Returns: inferred stochastic matrix.\n",
      "    \"\"\"\n",
      "    alpha0 = alphas.sum(axis=1, keepdims=True)\n",
      "    A = alphas - a\n",
      "    A[A < tol] = 0\n",
      "    A = A / (A.sum(axis=1, keepdims=True) + 1e-15)\n",
      "    return A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gensim corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "id2word = gensim.corpora.Dictionary.load_from_text('./wiki_corpus/enwiki-20141208-pages-articles_wordids.txt.bz2')\n",
      "bow_toy1 = gensim.corpora.MmCorpus('./wiki_corpus/wiki_bow_toy1.mm')\n",
      "bow_toy2 = gensim.corpora.MmCorpus('./wiki_corpus/wiki_bow_toy2.mm')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Toy1:', bow_toy1\n",
      "print 'Toy2:', bow_toy2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Toy1: MmCorpus(1000 documents, 100000 features, 157918 non-zero entries)\n",
        "Toy2: MmCorpus(1000 documents, 100000 features, 156568 non-zero entries)\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Run training/prediction"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls wiki_corpus/"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "enwiki-20141208-pages-articles_wordids.txt.bz2 wiki_bow_toy1.vw                               wiki_bow_toy2.vw\r\n",
        "wiki_bow_toy1.mm                               wiki_bow_toy2.mm\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw  \\\n",
      "    wiki_corpus/wiki_bow_toy1.vw -b 17 --cache_file /tmp/vw.cache \\\n",
      "    --lda 10 \\\n",
      "    --lda_alpha 0.1 \\\n",
      "    --lda_rho 0.1 \\\n",
      "    --lda_D 1000 \\\n",
      "    --minibatch 100 \\\n",
      "    --power_t 0.5 \\\n",
      "    --initial_t 1 \\\n",
      "    --readable_model vw_topics \\\n",
      "    --passes 10 \\\n",
      "    -f vw_model \\\n",
      "    -p vw_predictions_toy1 \\"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Num weight bits = 17\n",
        "learning rate = 0.5\n",
        "initial_t = 1\n",
        "power_t = 0.5\n",
        "decay_learning_rate = 1\n",
        "final_regressor = vw_model\n",
        "predictions = vw_predictions_toy1\n",
        "using cache_file = /tmp/vw.cache\n",
        "ignoring text input in favor of cache input\n",
        "num sources = 1\n",
        "average    since         example     example  current  current  current\n",
        "loss       last          counter      weight    label  predict features\n",
        "13.376221  13.376221           1         1.0  unknown   0.0000      119\n",
        "13.353944  13.331668           2         2.0  unknown   0.0000       64\n",
        "13.375204  13.396464           4         4.0  unknown   0.0000       63\n",
        "13.371108  13.367013           8         8.0  unknown   0.0000       49\n",
        "14.168797  14.966486          16        16.0  unknown   0.0000      352\n",
        "14.611538  15.054280          32        32.0  unknown   0.0000      324\n",
        "14.834853  15.058168          64        64.0  unknown   0.0000       30\n",
        "14.230720  13.626588         128       128.0  unknown   0.0000      327\n",
        "13.442233  12.653745         256       256.0  unknown   0.0000      131\n",
        "12.872881  12.303529         512       512.0  unknown   0.0000       82\n",
        "12.348856  11.824831        1024      1024.0  unknown   0.0000      269\n",
        "11.645740  10.942623        2048      2048.0  unknown   0.0000       28\n",
        "11.157371  10.669002        4096      4096.0  unknown   0.0000       20\n",
        "10.841038  10.524705        8192      8192.0  unknown   0.0000      139\n",
        "\n",
        "finished run\n",
        "number of examples = 9000\n",
        "weighted example sum = 9000\n",
        "weighted label sum = 0\n",
        "average loss = 0 h\n",
        "best constant = 0.000999001\n",
        "best constant's loss = 0.000998003\n",
        "total feature number = 1436170\n"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!vw \\\n",
      "    wiki_corpus/wiki_bow_toy2.vw \\\n",
      "    --lda 10 \\\n",
      "    --minibatch 1000 \\\n",
      "    --initial_regressor vw_model \\\n",
      "    -p vw_predictions_toy2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Num weight bits = 17\n",
        "learning rate = 0.5\n",
        "initial_t = 0\n",
        "power_t = 0.5\n",
        "predictions = vw_predictions_toy2\n",
        "using no cache\n",
        "Reading datafile = wiki_corpus/wiki_bow_toy2.vw\n",
        "num sources = 1\n",
        "average    since         example     example  current  current  current\n",
        "loss       last          counter      weight    label  predict features\n",
        "10.879272  10.879272           1         1.0  unknown   0.0000      234\n",
        "10.360690  9.842108            2         2.0  unknown   0.0000      181\n",
        "10.631879  10.903067           4         4.0  unknown   0.0000       43\n",
        "10.596480  10.561082           8         8.0  unknown   0.0000      169\n",
        "10.496013  10.395545          16        16.0  unknown   0.0000       69\n",
        "10.695209  10.894406          32        32.0  unknown   0.0000      177\n",
        "10.757591  10.819972          64        64.0  unknown   0.0000       82\n",
        "10.784241  10.810892         128       128.0  unknown   0.0000      229\n",
        "10.787811  10.791381         256       256.0  unknown   0.0000      397\n",
        "10.797498  10.807185         512       512.0  unknown   0.0000       80\n",
        "\n",
        "finished run\n",
        "number of examples = 1000\n",
        "weighted example sum = 1000\n",
        "weighted label sum = 0\n",
        "average loss = 10.8261\n",
        "best constant = nan\n",
        "total feature number = 156568\n"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lambdas = read_vw_lambdas('./vw_topics', n_term=bow_toy1.num_terms)\n",
      "gammas_toy1 = read_vw_gammas('./vw_predictions_toy1')\n",
      "gammas_toy2 = read_vw_gammas('./vw_predictions_toy2')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "a = 0\n",
      "\n",
      "Phi = infer_factorization(lambdas, a)\n",
      "Theta_toy1 = infer_factorization(gammas_toy1, a)\n",
      "Theta_toy2 = infer_factorization(gammas_toy2, a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Phi.shape, Theta_toy1.shape, Theta_toy2.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 150,
       "text": [
        "((10, 100000), (10000, 10), (1000, 10))"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "def perplexity(self, Theta, Phi):\n",
      "    F = self.F\n",
      "    phat = (Theta[F.row, :] * Phi.T[F.col, :]).sum(axis=1)\n",
      "    L = (F.data * np.log(phat)).sum() / F.data.sum()\n",
      "    return np.exp(-L)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def compute_perplexity(bow, Phi, Theta):\n",
      "    sum_n = 0.0\n",
      "    sum_loglike = 0.0\n",
      "    for doc_id, doc in enumerate(bow_toy2):\n",
      "        for term_id, count in doc:\n",
      "            sum_n += count\n",
      "            sum_loglike += count * np.log( np.dot(Theta[doc_id, :], Phi[:, term_id]) )\n",
      "    perplex = np.exp(-sum_loglike / sum_n)\n",
      "    return perplex    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print compute_perplexity(bow_toy1, Phi, Theta_toy1)\n",
      "print compute_perplexity(bow_toy1, Phi, Theta_toy2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "25349.7096241\n",
        "16076.7163873\n"
       ]
      }
     ],
     "prompt_number": 152
    }
   ],
   "metadata": {}
  }
 ]
}